{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b7e2c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\darknet\\darknet_importer.cpp:210: error: (-212:Parsing error) Failed to open NetParameter file: yolov3.cfg in function 'cv::dnn::dnn4_v20240521::readNetFromDarknet'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3684\\1443652977.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Load YOLO model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"yolov3.weights\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"yolov3.cfg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"coco.names\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\darknet\\darknet_importer.cpp:210: error: (-212:Parsing error) Failed to open NetParameter file: yolov3.cfg in function 'cv::dnn::dnn4_v20240521::readNetFromDarknet'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import speech_recognition as sr\n",
    "import threading\n",
    "\n",
    "# Constants\n",
    "REAL_WORLD_OBJECT_HEIGHT = 1.7  # Average height of a person in meters\n",
    "FOCAL_LENGTH = 800  # Example focal length in pixels; calibrate your camera for accurate results\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Initialize speech recognition\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def detect_objects(frame):\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # Preprocess the frame for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    detected_objects = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            for obj in detection:\n",
    "                obj = np.array(obj)\n",
    "                \n",
    "                # Ensure obj has enough elements\n",
    "                if obj.size >= 6:\n",
    "                    # Extract bounding box coordinates and class scores\n",
    "                    center_x, center_y, w, h = obj[0:4] * [width, height, width, height]\n",
    "                    scores = obj[5:]\n",
    "                    class_id = np.argmax(scores)\n",
    "                    confidence = scores[class_id]\n",
    "\n",
    "                    if confidence > 0.5:\n",
    "                        x = int(center_x - w / 2)\n",
    "                        y = int(center_y - h / 2)\n",
    "\n",
    "                        # Calculate distance\n",
    "                        object_size = h  # or use w if more appropriate\n",
    "                        distance = (REAL_WORLD_OBJECT_HEIGHT * FOCAL_LENGTH) / object_size\n",
    "\n",
    "                        label = str(classes[class_id])\n",
    "                        detected_objects.append((label, distance, x, y, w, h))\n",
    "    \n",
    "    return detected_objects\n",
    "\n",
    "def speak_detected_objects(objects, frame):\n",
    "    if not objects:\n",
    "        engine.say(\"No new objects detected.\")\n",
    "    else:\n",
    "        for obj in objects:\n",
    "            label, distance, x, y, w, h = obj\n",
    "            engine.say(f\"I see a {label} at approximately {distance:.2f} meters away.\")\n",
    "            # Draw bounding boxes on the frame\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{label} {distance:.2f}m\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def voice_command():\n",
    "    while True:\n",
    "        with sr.Microphone() as source:\n",
    "            print(\"Listening for commands...\")\n",
    "            recognizer.adjust_for_ambient_noise(source)\n",
    "            audio = recognizer.listen(source)\n",
    "\n",
    "            try:\n",
    "                command = recognizer.recognize_google(audio).lower()\n",
    "                print(\"Command:\", command)\n",
    "                if command == \"stop\":\n",
    "                    return\n",
    "            except sr.UnknownValueError:\n",
    "                pass  # Ignore unrecognized commands\n",
    "            except sr.RequestError as e:\n",
    "                print(f\"Google Speech Recognition request failed: {e}\")\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture.\")\n",
    "    exit()\n",
    "\n",
    "detected_objects = []\n",
    "\n",
    "# Start voice command thread\n",
    "command_thread = threading.Thread(target=voice_command, daemon=True)\n",
    "command_thread.start()\n",
    "\n",
    "# Start object detection\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # Perform object detection\n",
    "    new_objects = detect_objects(frame)\n",
    "\n",
    "    # Speak out detected objects with distance\n",
    "    speak_detected_objects(new_objects, frame)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Object Detection\", frame)\n",
    "\n",
    "    # Break the loop if the command thread has requested to stop\n",
    "    if not command_thread.is_alive():\n",
    "        break\n",
    "\n",
    "# Release the video capture and close the text-to-speech engine\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "engine.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall pyaudio\n",
    "%pip install pyaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb65ac0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping opencv-python as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall opencv-python\n",
    "%pip uninstall opencv-python-headless\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc76a1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "     ---------------------------------------- 38.8/38.8 MB 1.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17.0 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from opencv-python) (1.21.5)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.10.0.84\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "863ed6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in d:\\users\\admin\\anaconda3\\lib\\site-packages (3.5.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a732dc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\darknet\\darknet_importer.cpp:210: error: (-212:Parsing error) Failed to open NetParameter file: yolov3.cfg in function 'cv::dnn::dnn4_v20240521::readNetFromDarknet'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3684\\1312675005.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Load YOLO model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"yolov3.weights\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"yolov3.cfg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"coco.names\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\darknet\\darknet_importer.cpp:210: error: (-212:Parsing error) Failed to open NetParameter file: yolov3.cfg in function 'cv::dnn::dnn4_v20240521::readNetFromDarknet'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import speech_recognition as sr\n",
    "import threading\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Constants\n",
    "REAL_WORLD_OBJECT_HEIGHT = 1.7  # Average height of a person in meters\n",
    "FOCAL_LENGTH = 800  # Example focal length in pixels; calibrate your camera for accurate results\n",
    "CONFIDENCE_THRESHOLD = 0.3  # Lowered confidence threshold\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Initialize speech recognition\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def detect_objects(frame):\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # Preprocess the frame for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    detected_objects = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            for obj in detection:\n",
    "                obj = np.array(obj)\n",
    "\n",
    "                # Ensure obj has enough elements\n",
    "                if obj.size >= 6:\n",
    "                    # Extract bounding box coordinates and class scores\n",
    "                    center_x, center_y, w, h = obj[0:4] * [width, height, width, height]\n",
    "                    scores = obj[5:]\n",
    "                    class_id = np.argmax(scores)\n",
    "                    confidence = scores[class_id]\n",
    "\n",
    "                    if confidence > CONFIDENCE_THRESHOLD:\n",
    "                        x = int(center_x - w / 2)\n",
    "                        y = int(center_y - h / 2)\n",
    "\n",
    "                        # Calculate distance\n",
    "                        object_size = h  # or use w if more appropriate\n",
    "                        distance = (REAL_WORLD_OBJECT_HEIGHT * FOCAL_LENGTH) / object_size\n",
    "\n",
    "                        label = str(classes[class_id])\n",
    "                        detected_objects.append((label, distance, x, y, w, h))\n",
    "    \n",
    "    return detected_objects\n",
    "\n",
    "def speak_detected_objects(objects, frame):\n",
    "    if not objects:\n",
    "        engine.say(\"No objects detected.\")\n",
    "    else:\n",
    "        for obj in objects:\n",
    "            label, distance, x, y, w, h = obj\n",
    "            engine.say(f\"I see a {label} at approximately {distance:.2f} meters away.\")\n",
    "            # Draw bounding boxes on the frame\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{label} {distance:.2f}m\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def show_frame(frame):\n",
    "    # Convert the frame from BGR to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.imshow(rgb_frame)\n",
    "    plt.axis('off')  # Hide the axis\n",
    "    plt.show()\n",
    "\n",
    "def voice_command():\n",
    "    while True:\n",
    "        with sr.Microphone() as source:\n",
    "            print(\"Listening for commands...\")\n",
    "            recognizer.adjust_for_ambient_noise(source)\n",
    "            audio = recognizer.listen(source)\n",
    "\n",
    "            try:\n",
    "                command = recognizer.recognize_google(audio).lower()\n",
    "                print(\"Command:\", command)\n",
    "                if command == \"stop\":\n",
    "                    return\n",
    "            except sr.UnknownValueError:\n",
    "                pass  # Ignore unrecognized commands\n",
    "            except sr.RequestError as e:\n",
    "                print(f\"Google Speech Recognition request failed: {e}\")\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture.\")\n",
    "    exit()\n",
    "\n",
    "detected_objects = []\n",
    "\n",
    "# Start voice command thread\n",
    "command_thread = threading.Thread(target=voice_command, daemon=True)\n",
    "command_thread.start()\n",
    "\n",
    "# Start object detection\n",
    "frame_count = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # Perform object detection\n",
    "    new_objects = detect_objects(frame)\n",
    "\n",
    "    # Debugging: Print detected objects\n",
    "    if not new_objects:\n",
    "        print(\"No objects detected.\")\n",
    "    else:\n",
    "        for obj in new_objects:\n",
    "            print(f\"Detected: {obj[0]} at distance {obj[1]:.2f}m\")\n",
    "\n",
    "    # Speak out detected objects with distance\n",
    "    speak_detected_objects(new_objects, frame)\n",
    "\n",
    "    # Display the frame\n",
    "    show_frame(frame)\n",
    "\n",
    "    # Save the frame\n",
    "    filename = f\"frame_{frame_count}.jpg\"\n",
    "    cv2.imwrite(filename, frame)\n",
    "    print(f\"Saved {filename}\")\n",
    "    frame_count += 1\n",
    "\n",
    "    # Break the loop if the command thread has requested to stop\n",
    "    if not command_thread.is_alive():\n",
    "        break\n",
    "\n",
    "# Release the video capture and close the text-to-speech engine\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "engine.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784ee25d",
   "metadata": {},
   "source": [
    "# Gives Frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5906b9b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3684\\4130540530.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Paths to the YOLO files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mbase_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Directory of the script\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mcfg_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"yolov3.cfg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mweights_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"yolov3.weights\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import speech_recognition as sr\n",
    "import threading\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Constants\n",
    "REAL_WORLD_OBJECT_HEIGHT = 1.7  # Average height of a person in meters\n",
    "FOCAL_LENGTH = 800  # Example focal length in pixels; calibrate your camera for accurate results\n",
    "CONFIDENCE_THRESHOLD = 0.3  # Lowered confidence threshold\n",
    "\n",
    "# Paths to the YOLO files\n",
    "base_path = os.path.dirname(__file__)  # Directory of the script\n",
    "cfg_path = os.path.join(base_path, \"yolov3.cfg\")\n",
    "weights_path = os.path.join(base_path, \"yolov3.weights\")\n",
    "names_path = os.path.join(base_path, \"coco.names\")\n",
    "\n",
    "# Check if files exist\n",
    "if not os.path.isfile(cfg_path):\n",
    "    raise FileNotFoundError(f\"Configuration file not found: {cfg_path}\")\n",
    "if not os.path.isfile(weights_path):\n",
    "    raise FileNotFoundError(f\"Weights file not found: {weights_path}\")\n",
    "if not os.path.isfile(names_path):\n",
    "    raise FileNotFoundError(f\"Names file not found: {names_path}\")\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(weights_path, cfg_path)\n",
    "with open(names_path, \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Initialize speech recognition\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def detect_objects(frame):\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # Preprocess the frame for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    detected_objects = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            for obj in detection:\n",
    "                obj = np.array(obj)\n",
    "\n",
    "                # Ensure obj has enough elements\n",
    "                if obj.size >= 6:\n",
    "                    # Extract bounding box coordinates and class scores\n",
    "                    center_x, center_y, w, h = obj[0:4] * [width, height, width, height]\n",
    "                    scores = obj[5:]\n",
    "                    class_id = np.argmax(scores)\n",
    "                    confidence = scores[class_id]\n",
    "\n",
    "                    if confidence > CONFIDENCE_THRESHOLD:\n",
    "                        x = int(center_x - w / 2)\n",
    "                        y = int(center_y - h / 2)\n",
    "\n",
    "                        # Calculate distance\n",
    "                        object_size = h  # or use w if more appropriate\n",
    "                        distance = (REAL_WORLD_OBJECT_HEIGHT * FOCAL_LENGTH) / object_size\n",
    "\n",
    "                        label = str(classes[class_id])\n",
    "                        detected_objects.append((label, distance, x, y, w, h))\n",
    "    \n",
    "    return detected_objects\n",
    "\n",
    "def speak_detected_objects(objects, frame):\n",
    "    if not objects:\n",
    "        engine.say(\"No objects detected.\")\n",
    "    else:\n",
    "        for obj in objects:\n",
    "            label, distance, x, y, w, h = obj\n",
    "            engine.say(f\"I see a {label} at approximately {distance:.2f} meters away.\")\n",
    "            # Draw bounding boxes on the frame\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{label} {distance:.2f}m\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def show_frame(frame):\n",
    "    # Convert the frame from BGR to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.imshow(rgb_frame)\n",
    "    plt.axis('off')  # Hide the axis\n",
    "    plt.show()\n",
    "\n",
    "def voice_command():\n",
    "    while True:\n",
    "        with sr.Microphone() as source:\n",
    "            print(\"Listening for commands...\")\n",
    "            recognizer.adjust_for_ambient_noise(source)\n",
    "            audio = recognizer.listen(source)\n",
    "\n",
    "            try:\n",
    "                command = recognizer.recognize_google(audio).lower()\n",
    "                print(\"Command:\", command)\n",
    "                if command == \"stop\":\n",
    "                    return\n",
    "            except sr.UnknownValueError:\n",
    "                pass  # Ignore unrecognized commands\n",
    "            except sr.RequestError as e:\n",
    "                print(f\"Google Speech Recognition request failed: {e}\")\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture.\")\n",
    "    exit()\n",
    "\n",
    "detected_objects = []\n",
    "\n",
    "# Start voice command thread\n",
    "command_thread = threading.Thread(target=voice_command, daemon=True)\n",
    "command_thread.start()\n",
    "\n",
    "# Start object detection\n",
    "frame_count = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # Perform object detection\n",
    "    new_objects = detect_objects(frame)\n",
    "\n",
    "    # Debugging: Print detected objects\n",
    "    if not new_objects:\n",
    "        print(\"No objects detected.\")\n",
    "    else:\n",
    "        for obj in new_objects:\n",
    "            print(f\"Detected: {obj[0]} at distance {obj[1]:.2f}m\")\n",
    "\n",
    "    # Speak out detected objects with distance\n",
    "    speak_detected_objects(new_objects, frame)\n",
    "\n",
    "    # Display the frame\n",
    "    show_frame(frame)\n",
    "\n",
    "    # Save the frame\n",
    "    filename = f\"frame_{frame_count}.jpg\"\n",
    "    cv2.imwrite(filename, frame)\n",
    "    print(f\"Saved {filename}\")\n",
    "    frame_count += 1\n",
    "\n",
    "    # Break the loop if the command thread has requested to stop\n",
    "    if not command_thread.is_alive():\n",
    "        break\n",
    "\n",
    "# Release the video capture and close the text-to-speech engine\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "engine.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd69ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import speech_recognition as sr\n",
    "import threading\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Constants\n",
    "REAL_WORLD_OBJECT_HEIGHT = 1.7  # Average height of a person in meters\n",
    "FOCAL_LENGTH = 800  # Example focal length in pixels; calibrate your camera for accurate results\n",
    "CONFIDENCE_THRESHOLD = 0.3  # Lowered confidence threshold\n",
    "\n",
    "# Paths to the YOLO files\n",
    "cfg_path = \"yolov3.cfg\"\n",
    "weights_path = \"yolov3.weights\"\n",
    "names_path = \"coco.names\"\n",
    "\n",
    "# Check if files exist\n",
    "if not os.path.isfile(cfg_path):\n",
    "    raise FileNotFoundError(f\"Configuration file not found: {cfg_path}\")\n",
    "if not os.path.isfile(weights_path):\n",
    "    raise FileNotFoundError(f\"Weights file not found: {weights_path}\")\n",
    "if not os.path.isfile(names_path):\n",
    "    raise FileNotFoundError(f\"Names file not found: {names_path}\")\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(weights_path, cfg_path)\n",
    "with open(names_path, \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Initialize speech recognition\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def detect_objects(frame):\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # Preprocess the frame for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    detected_objects = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            for obj in detection:\n",
    "                obj = np.array(obj)\n",
    "\n",
    "                # Ensure obj has enough elements\n",
    "                if obj.size >= 6:\n",
    "                    # Extract bounding box coordinates and class scores\n",
    "                    center_x, center_y, w, h = obj[0:4] * [width, height, width, height]\n",
    "                    scores = obj[5:]\n",
    "                    class_id = np.argmax(scores)\n",
    "                    confidence = scores[class_id]\n",
    "\n",
    "                    if confidence > CONFIDENCE_THRESHOLD:\n",
    "                        x = int(center_x - w / 2)\n",
    "                        y = int(center_y - h / 2)\n",
    "\n",
    "                        # Calculate distance\n",
    "                        object_size = h  # or use w if more appropriate\n",
    "                        distance = (REAL_WORLD_OBJECT_HEIGHT * FOCAL_LENGTH) / object_size\n",
    "\n",
    "                        label = str(classes[class_id])\n",
    "                        detected_objects.append((label, distance, x, y, w, h))\n",
    "    \n",
    "    return detected_objects\n",
    "\n",
    "def speak_detected_objects(objects, frame):\n",
    "    if not objects:\n",
    "        engine.say(\"No objects detected.\")\n",
    "    else:\n",
    "        for obj in objects:\n",
    "            label, distance, x, y, w, h = obj\n",
    "            engine.say(f\"I see a {label} at approximately {distance:.2f} meters away.\")\n",
    "            # Draw bounding boxes on the frame\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{label} {distance:.2f}m\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def show_frame(frame):\n",
    "    # Convert the frame from BGR to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.imshow(rgb_frame)\n",
    "    plt.axis('off')  # Hide the axis\n",
    "    plt.show()\n",
    "\n",
    "def voice_command():\n",
    "    while True:\n",
    "        with sr.Microphone() as source:\n",
    "            print(\"Listening for commands...\")\n",
    "            recognizer.adjust_for_ambient_noise(source)\n",
    "            audio = recognizer.listen(source)\n",
    "\n",
    "            try:\n",
    "                command = recognizer.recognize_google(audio).lower()\n",
    "                print(\"Command:\", command)\n",
    "                if command == \"stop\":\n",
    "                    return\n",
    "            except sr.UnknownValueError:\n",
    "                pass  # Ignore unrecognized commands\n",
    "            except sr.RequestError as e:\n",
    "                print(f\"Google Speech Recognition request failed: {e}\")\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture.\")\n",
    "    exit()\n",
    "\n",
    "detected_objects = []\n",
    "\n",
    "# Start voice command thread\n",
    "command_thread = threading.Thread(target=voice_command, daemon=True)\n",
    "command_thread.start()\n",
    "\n",
    "# Start object detection\n",
    "frame_count = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # Perform object detection\n",
    "    new_objects = detect_objects(frame)\n",
    "\n",
    "    # Debugging: Print detected objects\n",
    "    if not new_objects:\n",
    "        print(\"No objects detected.\")\n",
    "    else:\n",
    "        for obj in new_objects:\n",
    "            print(f\"Detected: {obj[0]} at distance {obj[1]:.2f}m\")\n",
    "\n",
    "    # Speak out detected objects with distance\n",
    "    speak_detected_objects(new_objects, frame)\n",
    "\n",
    "    # Display the frame\n",
    "    show_frame(frame)\n",
    "\n",
    "    # Save the frame\n",
    "    filename = f\"frame_{frame_count}.jpg\"\n",
    "    cv2.imwrite(filename, frame)\n",
    "    print(f\"Saved {filename}\")\n",
    "    frame_count += 1\n",
    "\n",
    "    # Break the loop if the command thread has requested to stop\n",
    "    if not command_thread.is_alive():\n",
    "        break\n",
    "\n",
    "# Release the video capture and close the text-to-speech engine\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "engine.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9732117d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
