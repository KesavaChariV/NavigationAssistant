{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b65db1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.9.0.80-cp37-abi3-win_amd64.whl (38.6 MB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from opencv-python) (1.21.5)\n",
      "Installing collected packages: opencv-python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'D:\\\\Users\\\\Admin\\\\anaconda3\\\\Lib\\\\site-packages\\\\cv2\\\\cv2.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyttsx3 in d:\\users\\admin\\anaconda3\\lib\\site-packages (2.90)\n",
      "Requirement already satisfied: pypiwin32 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from pyttsx3) (223)\n",
      "Requirement already satisfied: comtypes in d:\\users\\admin\\anaconda3\\lib\\site-packages (from pyttsx3) (1.1.10)\n",
      "Requirement already satisfied: pywin32 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from pyttsx3) (302)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install pyttsx3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90f1020b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (155293120.py, line 60)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_11940\\155293120.py\"\u001b[1;36m, line \u001b[1;32m60\u001b[0m\n\u001b[1;33m    cv2.putText(image, f\"{label} {confidence:.2f}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX,\u001b[0m\n\u001b[1;37m                                                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "\n",
    "# Load YOLOv3 model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "def detect_objects(image):\n",
    "    height, width, channels = image.shape\n",
    "\n",
    "    # Preprocess the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                class_ids.append(class_id)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([x, y, w, h])\n",
    "\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    detected_objects = []\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indices:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            detected_objects.append(label)\n",
    "            confidence = confidences[i]\n",
    "            color = (0, 255, 0)\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(image, f\"{label} {confidence:.2f}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf22769e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10420\\4064611207.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;31m# Detect objects in the image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdetected_objects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;31m# Speak out the detected objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10420\\4064611207.py\u001b[0m in \u001b[0;36mdetect_objects\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdetect_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# Preprocess the image for YOLO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "\n",
    "# Load YOLOv3 model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "def detect_objects(image):\n",
    "    height, width, channels = image.shape\n",
    "\n",
    "    # Preprocess the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                class_ids.append(class_id)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([x, y, w, h])\n",
    "\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    detected_objects = []\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indices:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            detected_objects.append(label)\n",
    "            confidence = confidences[i]\n",
    "            color = (0, 255, 0)\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(image, f\"{label} {confidence:.2f}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    return image, detected_objects\n",
    "\n",
    "def speak_detected_objects(objects):\n",
    "    for obj in objects:\n",
    "        engine.say(f\"I see a {obj}\")\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Load an image\n",
    "image_path = 'ios-13-stock-ipados-red-black-background-amoled-hd-3208x3208-799.jpg'  # Replace with the path to your image\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Detect objects in the image\n",
    "image, detected_objects = detect_objects(image)\n",
    "\n",
    "# Speak out the detected objects\n",
    "if detected_objects:\n",
    "    speak_detected_objects(detected_objects)\n",
    "else:\n",
    "    engine.say(\"No objects detected\")\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Save the image to a file\n",
    "cv2.imwrite('detected_objects.jpg', image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19429f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "\n",
    "# Load YOLOv3 model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "def detect_objects(image):\n",
    "    height, width, channels = image.shape\n",
    "\n",
    "    # Preprocess the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                class_ids.append(class_id)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([x, y, w, h])\n",
    "\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    detected_objects = []\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indices:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            detected_objects.append(label)\n",
    "            confidence = confidences[i]\n",
    "            color = (0, 255, 0)\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(image, f\"{label} {confidence:.2f}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    return image, detected_objects\n",
    "\n",
    "def speak_detected_objects(objects):\n",
    "    for obj in objects:\n",
    "        engine.say(f\"I see a {obj}\")\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detect objects in the frame\n",
    "    frame, detected_objects = detect_objects(frame)\n",
    "\n",
    "    # Speak out the detected objects\n",
    "    if detected_objects:\n",
    "        speak_detected_objects(detected_objects)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Object Detection\", frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c2d6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "\n",
    "# Load YOLOv3 model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "def detect_objects(image):\n",
    "    height, width, channels = image.shape\n",
    "\n",
    "    # Preprocess the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                class_ids.append(class_id)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([x, y, w, h])\n",
    "\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    detected_objects = []\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indices:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            detected_objects.append(label)\n",
    "            confidence = confidences[i]\n",
    "            color = (0, 255, 0)\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(image, f\"{label} {confidence:.2f}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    return image, detected_objects\n",
    "\n",
    "def speak_detected_objects(objects):\n",
    "    for obj in objects:\n",
    "        engine.say(f\"I see a {obj}\")\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detect objects in the frame\n",
    "    frame, detected_objects = detect_objects(frame)\n",
    "\n",
    "    # Speak out the detected objects\n",
    "    if detected_objects:\n",
    "        speak_detected_objects(detected_objects)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6593feec",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1266: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvDestroyAllWindows'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11540\\3406840104.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;31m# Release the webcam and close all windows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1266: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvDestroyAllWindows'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "\n",
    "# Load YOLOv3 model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "def detect_objects(image):\n",
    "    height, width, channels = image.shape\n",
    "\n",
    "    # Preprocess the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                class_ids.append(class_id)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([x, y, w, h])\n",
    "\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    detected_objects = []\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indices:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            detected_objects.append(label)\n",
    "            confidence = confidences[i]\n",
    "            color = (0, 255, 0)\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(image, f\"{label} {confidence:.2f}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    return image, detected_objects\n",
    "\n",
    "def speak_detected_objects(objects):\n",
    "    for obj in objects:\n",
    "        engine.say(f\"I see a {obj}\")\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detect objects in the frame\n",
    "    frame, detected_objects = detect_objects(frame)\n",
    "\n",
    "    # Speak out the detected objects\n",
    "    if detected_objects:\n",
    "        speak_detected_objects(detected_objects)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Object Detection\", frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c012745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "\n",
    "# Load YOLOv3 model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "def detect_objects(image):\n",
    "    height, width, channels = image.shape\n",
    "\n",
    "    # Preprocess the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                class_ids.append(class_id)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([x, y, w, h])\n",
    "\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    detected_objects = []\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indices:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            detected_objects.append(label)\n",
    "            confidence = confidences[i]\n",
    "            color = (0, 255, 0)\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(image, f\"{label} {confidence:.2f}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    return image, detected_objects\n",
    "\n",
    "def speak_detected_objects(objects):\n",
    "    for obj in objects:\n",
    "        engine.say(f\"I see a {obj}\")\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detect objects in the frame\n",
    "    frame, detected_objects = detect_objects(frame)\n",
    "\n",
    "    # Speak out the detected objects\n",
    "    if detected_objects:\n",
    "        speak_detected_objects(detected_objects)\n",
    "\n",
    "# Release the webcam\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f8edea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e099c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Load YOLOv3 model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Initialize speech recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def detect_objects(image):\n",
    "    height, width, channels = image.shape\n",
    "\n",
    "    # Preprocess the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                class_ids.append(class_id)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([x, y, w, h])\n",
    "\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    detected_objects = []\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indices:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            detected_objects.append(label)\n",
    "            confidence = confidences[i]\n",
    "            color = (0, 255, 0)\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(image, f\"{label} {confidence:.2f}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    return image, detected_objects\n",
    "\n",
    "def speak_detected_objects(objects):\n",
    "    for obj in objects:\n",
    "        engine.say(f\"I see a {obj}\")\n",
    "    engine.runAndWait()\n",
    "\n",
    "def start_object_detection():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        # Read a frame from the webcam\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Detect objects in the frame\n",
    "        frame, detected_objects = detect_objects(frame)\n",
    "\n",
    "        # Speak out the detected objects\n",
    "        if detected_objects:\n",
    "            speak_detected_objects(detected_objects)\n",
    "\n",
    "            # Ensure that detected_image is a NumPy array\n",
    "        detected_image = np.array(detected_objects)\n",
    "\n",
    "# Check if detected_image is a valid NumPy array\n",
    "        if detected_image is not None:\n",
    "            # Save the image to a file\n",
    "            cv2.imwrite('detected_objects.jpg', frame)\n",
    "        else:\n",
    "            print(\"Error: Detected image is not a valid NumPy array.\")\n",
    "\n",
    "#         cv2.imwrite('detected_objects.jpg', frame)\n",
    "#         cv2.imwrite('detected_objects.jpg', detected_objects)\n",
    "\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the webcam and close all windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def recognize_commands():\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening for commands...\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio = recognizer.listen(source)\n",
    "\n",
    "        try:\n",
    "            command = recognizer.recognize_google(audio).lower()\n",
    "            print(\"Command:\", command)\n",
    "            return command\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Could not understand audio.\")\n",
    "            return None\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Google Speech Recognition request failed: {e}\")\n",
    "            return None\n",
    "\n",
    "while True:\n",
    "    command = recognize_commands()\n",
    "    if command == \"start\":\n",
    "        start_object_detection()\n",
    "    elif command == \"stop\":\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c964c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n",
      "Command: start another folder\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Load YOLOv3 model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Initialize speech recognition\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def detect_objects(frame):\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Preprocess the frame for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    detected_objects = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:\n",
    "                label = str(classes[class_id])\n",
    "                detected_objects.append(label)\n",
    "\n",
    "    return detected_objects\n",
    "\n",
    "def speak_detected_objects(objects):\n",
    "    for obj in objects:\n",
    "        engine.say(f\"I see a {obj}\")\n",
    "    engine.runAndWait()\n",
    "\n",
    "def voice_command():\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening for commands...\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio = recognizer.listen(source)\n",
    "\n",
    "        try:\n",
    "            command = recognizer.recognize_google(audio).lower()\n",
    "            print(\"Command:\", command)\n",
    "            return command\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Could not understand audio.\")\n",
    "            return None\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Google Speech Recognition request failed: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize video capture (replace 0 with your desired camera index)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the camera opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture.\")\n",
    "    exit()\n",
    "\n",
    "# Wait for voice command to start object detection\n",
    "start_detected = False\n",
    "while not start_detected:\n",
    "    command = voice_command()\n",
    "    if command == \"start\":\n",
    "        start_detected = True\n",
    "\n",
    "# Start object detection\n",
    "detected_objects = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # Perform object detection\n",
    "    new_objects = detect_objects(frame)\n",
    "\n",
    "    # Check for newly detected objects\n",
    "    new_objects = [obj for obj in new_objects if obj not in detected_objects]\n",
    "    if new_objects:\n",
    "        detected_objects.extend(new_objects)\n",
    "        speak_detected_objects(new_objects)\n",
    "\n",
    "    # Check for voice command to stop object detection\n",
    "    command = voice_command()\n",
    "    if command == \"stop\":\n",
    "        break\n",
    "\n",
    "# Release the video capture and close the text-to-speech engine\n",
    "cap.release()\n",
    "engine.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7aa077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Load YOLOv3 model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Initialize speech recognition\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def detect_objects(frame):\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Preprocess the frame for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    detected_objects = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:\n",
    "                label = str(classes[class_id])\n",
    "                detected_objects.append(label)\n",
    "\n",
    "    return detected_objects\n",
    "\n",
    "def speak_detected_objects(objects):\n",
    "    if len(objects) == 0:\n",
    "        engine.say(\"No objects detected.\")\n",
    "    else:\n",
    "        object_str = \", \".join(objects)\n",
    "        engine.say(f\"I see {object_str}\")\n",
    "    engine.runAndWait()\n",
    "\n",
    "def voice_command():\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening for commands...\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio = recognizer.listen(source)\n",
    "\n",
    "        try:\n",
    "            command = recognizer.recognize_google(audio).lower()\n",
    "            print(\"Command:\", command)\n",
    "            return command\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Could not understand audio.\")\n",
    "            return None\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Google Speech Recognition request failed: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize video capture (replace 0 with your desired camera index)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the camera opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture.\")\n",
    "    exit()\n",
    "\n",
    "# Wait for voice command to start object detection\n",
    "start_detected = False\n",
    "while not start_detected:\n",
    "    command = voice_command()\n",
    "    if command == \"start\":\n",
    "        start_detected = True\n",
    "\n",
    "# Start object detection\n",
    "detected_objects = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # Perform object detection\n",
    "    new_objects = detect_objects(frame)\n",
    "\n",
    "    # Check for newly detected objects\n",
    "    new_objects = [obj for obj in new_objects if obj not in detected_objects]\n",
    "    if new_objects:\n",
    "        detected_objects.extend(new_objects)\n",
    "\n",
    "    # Check for voice command to stop object detection\n",
    "    command = voice_command()\n",
    "    if command == \"stop\":\n",
    "        break\n",
    "\n",
    "# Speak out all detected objects\n",
    "speak_detected_objects(detected_objects)\n",
    "\n",
    "# Release the video capture and close the text-to-speech engine\n",
    "cap.release()\n",
    "engine.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6920153f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n",
      "Command: framework which is not\n",
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n",
      "Command: attribute\n",
      "Listening for commands...\n",
      "Command: model red light area\n",
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n",
      "Command: oppo short names\n",
      "Listening for commands...\n",
      "Command: science based company\n",
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n",
      "Command: which language speech recognition\n",
      "Listening for commands...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Load YOLOv3 model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Initialize speech recognition\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def detect_objects(frame):\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Preprocess the frame for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    detected_objects = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:\n",
    "                label = str(classes[class_id])\n",
    "                detected_objects.append(label)\n",
    "\n",
    "    return detected_objects\n",
    "\n",
    "def speak_detected_objects(objects):\n",
    "    if len(objects) == 0:\n",
    "        engine.say(\"No objects detected.\")\n",
    "    else:\n",
    "        object_str = \", \".join(objects)\n",
    "        engine.say(f\"I see {object_str}\")\n",
    "    engine.runAndWait()\n",
    "\n",
    "def voice_command():\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening for commands...\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio = recognizer.listen(source)\n",
    "\n",
    "        try:\n",
    "            command = recognizer.recognize_google(audio).lower()\n",
    "            print(\"Command:\", command)\n",
    "            return command\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Could not understand audio.\")\n",
    "            return None\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Google Speech Recognition request failed: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize video capture (replace 0 with your desired camera index)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the camera opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture.\")\n",
    "    exit()\n",
    "\n",
    "# Start object detection\n",
    "detected_objects = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # Perform object detection\n",
    "    new_objects = detect_objects(frame)\n",
    "\n",
    "    # Check for newly detected objects\n",
    "    new_objects = [obj for obj in new_objects if obj not in detected_objects]\n",
    "    if new_objects:\n",
    "        detected_objects.extend(new_objects)\n",
    "        speak_detected_objects(new_objects)\n",
    "\n",
    "    # Check for voice command to stop object detection\n",
    "    command = voice_command()\n",
    "    if command == \"stop\":\n",
    "        break\n",
    "\n",
    "# Release the video capture and close the text-to-speech engine\n",
    "cap.release()\n",
    "engine.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ee759b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n",
      "Command: new object is detected\n",
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n",
      "Could not understand audio.\n",
      "Listening for commands...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Load YOLOv3 model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Initialize speech recognition\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def detect_objects(frame):\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Preprocess the frame for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    detected_objects = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:\n",
    "                label = str(classes[class_id])\n",
    "                detected_objects.append(label)\n",
    "\n",
    "    return detected_objects\n",
    "\n",
    "def speak_detected_objects(objects):\n",
    "    new_objects = set(objects) - set(detected_objects)\n",
    "    if len(new_objects) == 0:\n",
    "        engine.say(\"No new objects detected.\")\n",
    "    else:\n",
    "        object_str = \", \".join(new_objects)\n",
    "        engine.say(f\"I see {object_str}\")\n",
    "        detected_objects.extend(new_objects)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def voice_command():\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening for commands...\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio = recognizer.listen(source)\n",
    "\n",
    "        try:\n",
    "            command = recognizer.recognize_google(audio).lower()\n",
    "            print(\"Command:\", command)\n",
    "            return command\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Could not understand audio.\")\n",
    "            return None\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Google Speech Recognition request failed: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize video capture (replace 0 with your desired camera index)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the camera opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture.\")\n",
    "    exit()\n",
    "\n",
    "# Start object detection\n",
    "detected_objects = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # Perform object detection\n",
    "    new_objects = detect_objects(frame)\n",
    "\n",
    "    # Speak out new detected objects\n",
    "    speak_detected_objects(new_objects)\n",
    "\n",
    "    # Check for voice command to stop object detection\n",
    "    command = voice_command()\n",
    "    if command == \"stop\":\n",
    "        break\n",
    "\n",
    "# Release the video capture and close the text-to-speech engine\n",
    "cap.release()\n",
    "engine.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb91093f",
   "metadata": {},
   "source": [
    "# Working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a5515a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for commands...\n",
      "Listening for commands...\n",
      "Listening for commands...\n",
      "Listening for commands...\n",
      "Listening for commands...\n",
      "Listening for commands...\n",
      "Command: stop\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import speech_recognition as sr\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Load YOLOv3 model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Initialize speech recognition\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def detect_objects(frame):\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Preprocess the frame for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    detected_objects = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:\n",
    "                label = str(classes[class_id])\n",
    "                detected_objects.append(label)\n",
    "\n",
    "    return detected_objects\n",
    "\n",
    "def speak_detected_objects(objects):\n",
    "    global detected_objects\n",
    "    new_objects = set(objects) - set(detected_objects)\n",
    "    if not new_objects:\n",
    "        engine.say(\"No new objects detected.\")\n",
    "    else:\n",
    "        object_str = \", \".join(new_objects)\n",
    "        engine.say(f\"I see {object_str}\")\n",
    "        detected_objects.extend(new_objects)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def voice_command():\n",
    "    while True:\n",
    "        with sr.Microphone() as source:\n",
    "            print(\"Listening for commands...\")\n",
    "            recognizer.adjust_for_ambient_noise(source)\n",
    "            audio = recognizer.listen(source)\n",
    "\n",
    "            try:\n",
    "                command = recognizer.recognize_google(audio).lower()\n",
    "                print(\"Command:\", command)\n",
    "                if command == \"stop\":\n",
    "                    return\n",
    "            except sr.UnknownValueError:\n",
    "                pass  # Ignore unrecognized commands\n",
    "            except sr.RequestError as e:\n",
    "                print(f\"Google Speech Recognition request failed: {e}\")\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture.\")\n",
    "    exit()\n",
    "\n",
    "detected_objects = []\n",
    "\n",
    "# Start voice command thread\n",
    "command_thread = threading.Thread(target=voice_command, daemon=True)\n",
    "command_thread.start()\n",
    "\n",
    "# Start object detection\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # Perform object detection\n",
    "    new_objects = detect_objects(frame)\n",
    "\n",
    "    # Speak out new detected objects\n",
    "    speak_detected_objects(new_objects)\n",
    "\n",
    "    # Check if the command thread has requested to stop\n",
    "    if not command_thread.is_alive():\n",
    "        break\n",
    "\n",
    "# Release the video capture and close the text-to-speech engine\n",
    "cap.release()\n",
    "engine.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88685387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for commands...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_42320\\2330882022.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;31m# Perform object detection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m     \u001b[0mnew_objects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;31m# Speak out new detected objects with distance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_42320\\2330882022.py\u001b[0m in \u001b[0;36mdetect_objects\u001b[1;34m(frame)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdetection\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdetection\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m                 \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m                 \u001b[0mclass_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[0mconfidence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclass_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for commands...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import speech_recognition as sr\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Constants\n",
    "REAL_WORLD_OBJECT_HEIGHT = 1.7  # Average height of a person in meters\n",
    "FOCAL_LENGTH = 800  # Example focal length in pixels; calibrate your camera for accurate results\n",
    "\n",
    "# Load YOLOv3 model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Initialize speech recognition\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def detect_objects(frame):\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Preprocess the frame for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    detected_objects = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            for obj in detection:\n",
    "                scores = obj[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "\n",
    "                if confidence > 0.5:\n",
    "                    center_x = int(obj[0] * width)\n",
    "                    center_y = int(obj[1] * height)\n",
    "                    w = int(obj[2] * width)\n",
    "                    h = int(obj[3] * height)\n",
    "                    \n",
    "                    # Calculate bounding box coordinates\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "\n",
    "                    # Calculate distance\n",
    "                    object_size = h  # or use w if you prefer\n",
    "                    distance = (REAL_WORLD_OBJECT_HEIGHT * FOCAL_LENGTH) / object_size\n",
    "\n",
    "                    label = str(classes[class_id])\n",
    "                    detected_objects.append((label, distance, x, y, w, h))\n",
    "\n",
    "    return detected_objects\n",
    "\n",
    "\n",
    "def speak_detected_objects(objects):\n",
    "    global detected_objects\n",
    "    new_objects = set(obj[0] for obj in objects) - set(obj[0] for obj in detected_objects)\n",
    "    if not new_objects:\n",
    "        engine.say(\"No new objects detected.\")\n",
    "    else:\n",
    "        for obj in objects:\n",
    "            label, distance, x, y, w, h = obj\n",
    "            engine.say(f\"I see a {label} at approximately {distance:.2f} meters away.\")\n",
    "            # Optionally, draw bounding boxes on the frame\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{label} {distance:.2f}m\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    engine.runAndWait()\n",
    "\n",
    "\n",
    "def voice_command():\n",
    "    while True:\n",
    "        with sr.Microphone() as source:\n",
    "            print(\"Listening for commands...\")\n",
    "            recognizer.adjust_for_ambient_noise(source)\n",
    "            audio = recognizer.listen(source)\n",
    "\n",
    "            try:\n",
    "                command = recognizer.recognize_google(audio).lower()\n",
    "                print(\"Command:\", command)\n",
    "                if command == \"stop\":\n",
    "                    return\n",
    "            except sr.UnknownValueError:\n",
    "                pass  # Ignore unrecognized commands\n",
    "            except sr.RequestError as e:\n",
    "                print(f\"Google Speech Recognition request failed: {e}\")\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture.\")\n",
    "    exit()\n",
    "\n",
    "detected_objects = []\n",
    "\n",
    "# Start voice command thread\n",
    "command_thread = threading.Thread(target=voice_command, daemon=True)\n",
    "command_thread.start()\n",
    "\n",
    "# Start object detection\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # Perform object detection\n",
    "    new_objects = detect_objects(frame)\n",
    "\n",
    "    # Speak out new detected objects with distance\n",
    "    speak_detected_objects(new_objects)\n",
    "\n",
    "    # Check if the command thread has requested to stop\n",
    "    if not command_thread.is_alive():\n",
    "        break\n",
    "\n",
    "# Release the video capture and close the text-to-speech engine\n",
    "cap.release()\n",
    "engine.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81107ba3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for commands...\n",
      "Listening for commands...\n",
      "Listening for commands...\n",
      "Listening for commands...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import speech_recognition as sr\n",
    "import threading\n",
    "\n",
    "# Constants\n",
    "REAL_WORLD_OBJECT_HEIGHT = 1.7  # Average height of a person in meters\n",
    "FOCAL_LENGTH = 800  # Example focal length in pixels; calibrate your camera for accurate results\n",
    "\n",
    "# Load YOLOv3 model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Initialize speech recognition\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def detect_objects(frame):\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # Preprocess the frame for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    detected_objects = []\n",
    "\n",
    "    for out in outs:\n",
    "        # Each 'out' is an array of detections\n",
    "        for detection in out:\n",
    "            for obj in detection:\n",
    "                # Convert obj to an array to ensure correct indexing\n",
    "                obj = np.array(obj)\n",
    "                \n",
    "                # Ensure obj is an array with at least 6 elements\n",
    "                if obj.size >= 6:\n",
    "                    # Extract the objectness score and class scores\n",
    "                    scores = obj[5:]\n",
    "                    class_id = np.argmax(scores)\n",
    "                    confidence = scores[class_id]\n",
    "\n",
    "                    if confidence > 0.5:\n",
    "                        # Extract bounding box coordinates\n",
    "                        center_x = int(obj[0] * width)\n",
    "                        center_y = int(obj[1] * height)\n",
    "                        w = int(obj[2] * width)\n",
    "                        h = int(obj[3] * height)\n",
    "                        \n",
    "                        # Calculate bounding box coordinates\n",
    "                        x = int(center_x - w / 2)\n",
    "                        y = int(center_y - h / 2)\n",
    "\n",
    "                        # Calculate distance\n",
    "                        object_size = h  # You can also use w if more appropriate\n",
    "                        distance = (REAL_WORLD_OBJECT_HEIGHT * FOCAL_LENGTH) / object_size\n",
    "\n",
    "                        label = str(classes[class_id])\n",
    "                        detected_objects.append((label, distance, x, y, w, h))\n",
    "    \n",
    "    return detected_objects\n",
    "\n",
    "\n",
    "def speak_detected_objects(objects):\n",
    "    global detected_objects\n",
    "    new_objects = set(obj[0] for obj in objects) - set(obj[0] for obj in detected_objects)\n",
    "    if not new_objects:\n",
    "        engine.say(\"No new objects detected.\")\n",
    "    else:\n",
    "        for obj in objects:\n",
    "            label, distance, x, y, w, h = obj\n",
    "            engine.say(f\"I see a {label} at approximately {distance:.2f} meters away.\")\n",
    "            # Optionally, draw bounding boxes on the frame\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{label} {distance:.2f}m\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    engine.runAndWait()\n",
    "\n",
    "def voice_command():\n",
    "    while True:\n",
    "        with sr.Microphone() as source:\n",
    "            print(\"Listening for commands...\")\n",
    "            recognizer.adjust_for_ambient_noise(source)\n",
    "            audio = recognizer.listen(source)\n",
    "\n",
    "            try:\n",
    "                command = recognizer.recognize_google(audio).lower()\n",
    "                print(\"Command:\", command)\n",
    "                if command == \"stop\":\n",
    "                    return\n",
    "            except sr.UnknownValueError:\n",
    "                pass  # Ignore unrecognized commands\n",
    "            except sr.RequestError as e:\n",
    "                print(f\"Google Speech Recognition request failed: {e}\")\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture.\")\n",
    "    exit()\n",
    "\n",
    "detected_objects = []\n",
    "\n",
    "# Start voice command thread\n",
    "command_thread = threading.Thread(target=voice_command, daemon=True)\n",
    "command_thread.start()\n",
    "\n",
    "# Start object detection\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # Perform object detection\n",
    "    new_objects = detect_objects(frame)\n",
    "\n",
    "    # Speak out new detected objects with distance\n",
    "    speak_detected_objects(new_objects)\n",
    "\n",
    "    # Check if the command thread has requested to stop\n",
    "    if not command_thread.is_alive():\n",
    "        break\n",
    "\n",
    "# Release the video capture and close the text-to-speech engine\n",
    "cap.release()\n",
    "engine.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714c660d",
   "metadata": {},
   "source": [
    "# Updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bd1c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import speech_recognition as sr\n",
    "import threading\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Constants\n",
    "REAL_WORLD_OBJECT_HEIGHT = 1.7  # Average height of a person in meters\n",
    "FOCAL_LENGTH = 800  # Adjust this value based on calibration for accurate distance\n",
    "CONFIDENCE_THRESHOLD = 0.1  # Lowered confidence threshold for more detection\n",
    "NMS_THRESHOLD = 0.4  # Non-Maximum Suppression threshold\n",
    "\n",
    "# Paths to the YOLO files\n",
    "cfg_path = \"yolov3.cfg\"\n",
    "weights_path = \"yolov3.weights\"\n",
    "names_path = \"coco.names\"\n",
    "\n",
    "# Check if files exist\n",
    "if not os.path.isfile(cfg_path):\n",
    "    raise FileNotFoundError(f\"Configuration file not found: {cfg_path}\")\n",
    "if not os.path.isfile(weights_path):\n",
    "    raise FileNotFoundError(f\"Weights file not found: {weights_path}\")\n",
    "if not os.path.isfile(names_path):\n",
    "    raise FileNotFoundError(f\"Names file not found: {names_path}\")\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(weights_path, cfg_path)\n",
    "with open(names_path, \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Initialize speech recognition\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Define the calibration constants\n",
    "Real_Height_Calibration = 1.7  # Known height of the object (in meters, e.g., height of a person)\n",
    "Distance_Calibration = 3.0  # Known distance from the camera to the object (in meters)\n",
    "Object_Height_Calibration = 500  # Example object height in pixels in the calibration image\n",
    "\n",
    "# Calculate the correct focal length based on calibration\n",
    "FOCAL_LENGTH = (Object_Height_Calibration * Distance_Calibration) / Real_Height_Calibration\n",
    "\n",
    "def detect_objects(frame):\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # Preprocess the frame for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > CONFIDENCE_THRESHOLD:\n",
    "                center_x, center_y, w, h = detection[0:4] * np.array([width, height, width, height])\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, int(w), int(h)])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # Apply Non-Maximum Suppression\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, CONFIDENCE_THRESHOLD, NMS_THRESHOLD)\n",
    "\n",
    "    detected_objects = []\n",
    "    for i in indices.flatten():  # Use .flatten() to convert to a 1D array\n",
    "        box = boxes[i]\n",
    "        x, y, w, h = box\n",
    "        label = str(classes[class_ids[i]])\n",
    "\n",
    "        # Calculate distance using the calibrated focal length\n",
    "        distance = (Real_Height_Calibration * FOCAL_LENGTH) / h  # Updated formula using calibrated values\n",
    "        detected_objects.append((label, distance, x, y, w, h))\n",
    "\n",
    "    return detected_objects\n",
    "\n",
    "# Example of running the updated distance estimation logic\n",
    "# Start your video capture and detection logic here as before\n",
    "\n",
    "\n",
    "def speak_detected_objects(objects, frame):\n",
    "    if not objects:\n",
    "        engine.say(\"No objects detected.\")\n",
    "    else:\n",
    "        for obj in objects:\n",
    "            label, distance, x, y, w, h = obj\n",
    "            engine.say(f\"I see a {label} at approximately {distance:.2f} meters away.\")\n",
    "            # Draw bounding boxes on the frame\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{label} {distance:.2f}m\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    try:\n",
    "        engine.runAndWait()\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Text-to-speech error: {e}\")\n",
    "\n",
    "def show_frame(frame):\n",
    "    # Convert the frame from BGR to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(rgb_frame)\n",
    "    plt.axis('off')  # Hide the axis\n",
    "    plt.show()\n",
    "\n",
    "def voice_command():\n",
    "    while True:\n",
    "        with sr.Microphone() as source:\n",
    "            print(\"Listening for commands...\")\n",
    "            recognizer.adjust_for_ambient_noise(source)\n",
    "            audio = recognizer.listen(source)\n",
    "\n",
    "            try:\n",
    "                command = recognizer.recognize_google(audio).lower()\n",
    "                print(\"Command:\", command)\n",
    "                if command == \"stop\":\n",
    "                    return\n",
    "            except sr.UnknownValueError:\n",
    "                pass  # Ignore unrecognized commands\n",
    "            except sr.RequestError as e:\n",
    "                print(f\"Google Speech Recognition request failed: {e}\")\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video capture.\")\n",
    "    exit()\n",
    "\n",
    "detected_objects = []\n",
    "\n",
    "# Start voice command thread\n",
    "command_thread = threading.Thread(target=voice_command, daemon=True)\n",
    "command_thread.start()\n",
    "\n",
    "# Start object detection\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # Limit frame processing to once per second\n",
    "    if time.time() - start_time >= 1.0:\n",
    "        # Perform object detection\n",
    "        new_objects = detect_objects(frame)\n",
    "\n",
    "        # Debugging: Print detected objects\n",
    "        if not new_objects:\n",
    "            print(\"No objects detected.\")\n",
    "        else:\n",
    "            for obj in new_objects:\n",
    "                print(f\"Detected: {obj[0]} at distance {obj[1]:.2f}m\")\n",
    "\n",
    "        # Speak out detected objects with distance\n",
    "        speak_detected_objects(new_objects, frame)\n",
    "\n",
    "        # Display the frame\n",
    "        show_frame(frame)\n",
    "\n",
    "        # Save the frame\n",
    "        filename = f\"frame_{frame_count}.jpg\"\n",
    "        cv2.imwrite(filename, frame)\n",
    "        print(f\"Saved {filename}\")\n",
    "        frame_count += 1\n",
    "\n",
    "        # Update the start time\n",
    "        start_time = time.time()\n",
    "\n",
    "    # Break the loop if the command thread has requested to stop\n",
    "    if not command_thread.is_alive():\n",
    "        break\n",
    "\n",
    "# Release the video capture and close the text-to-speech engine\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "engine.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e844ebf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
